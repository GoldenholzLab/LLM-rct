{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has not been fully tested. Results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim_EMR_v2 import build_trial_set\n",
    "idCount = 120\n",
    "per_arm = 10\n",
    "num_chunks = 6\n",
    "for chunks in range(num_chunks):\n",
    "    build_trial_set(patients_per_arm = per_arm,placebo_efficacy = 0, drug_efficacy = 0.39, baseline_dur = 2, test_dur = 3,\n",
    "                    fname_raw = 'RCT2_raw',fname_sum = 'RCT2_sum',true_sum='RCT2_true',IDstart=idCount,noSum=False,rawTF=True,\n",
    "                    LLM_gen_name='llama3:8b')\n",
    "    idCount += per_arm*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time out!"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielgoldenholz/miniforge3/envs/deept2/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sim_EMR_v2 import build_trial_set, make_one_patient\n",
    "name = '123'\n",
    "idnum=1\n",
    "drug_name='abc1'\n",
    "baseline_sz=2\n",
    "test_sz=4\n",
    "baseline_dur=2\n",
    "test_dur=3\n",
    "selected_sx_str='sad,tired,cranky'\n",
    "noSum=True\n",
    "LLM_gen_name='llama3:8b'\n",
    "LLM_sum_name='mistral'\n",
    "arm=1\n",
    "R1,R2 = make_one_patient(name = name, idnum= idnum, drug_name=drug_name , baseline_sz=baseline_sz, \n",
    "                                test_sz=test_sz, baseline_dur=baseline_dur, \n",
    "                                test_dur=test_dur,selected_sx_str = selected_sx_str,noSum=noSum,\n",
    "                                LLM_gen_name=LLM_gen_name,LLM_sum_name=LLM_sum_name,arm=arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 529.11\n",
      "Total sample size required: 10582.30\n"
     ]
    }
   ],
   "source": [
    "# used for SourceSpikeNet\n",
    "from statsmodels.stats.power import FTestAnovaPower\n",
    "\n",
    "# Define the parameters for the power analysis\n",
    "effect_size = 0.20  # medium effect size\n",
    "alpha = 0.05  # significance level\n",
    "power = 0.8  # desired power\n",
    "num_groups = 20  # number of groups (e.g., 20 master IED generators)\n",
    "\n",
    "# Create an instance of the FTestAnovaPower class\n",
    "anova_power = FTestAnovaPower()\n",
    "\n",
    "# Perform the power analysis to calculate the required sample size per group\n",
    "required_sample_size_per_group = anova_power.solve_power(effect_size=effect_size, \n",
    "                                                         alpha=alpha, \n",
    "                                                         power=power, \n",
    "                                                         k_groups=num_groups)\n",
    "\n",
    "# Calculate the total sample size required\n",
    "total_sample_size = required_sample_size_per_group * num_groups\n",
    "\n",
    "# Print the results\n",
    "print(f\"Required sample size per group: {required_sample_size_per_group:.2f}\")\n",
    "print(f\"Total sample size required: {total_sample_size:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "Required sample size per group: 14.00\n",
      "Total sample size required: 28.00\n"
     ]
    }
   ],
   "source": [
    "# used for SourceSpikeNet\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# Define the parameters for the power analysis\n",
    "effect_size = (60-16) / 40  # calculated Cohen's d\n",
    "print(effect_size)\n",
    "alpha = 0.05  # significance level\n",
    "power = 0.8  # desired power\n",
    "num_groups = 2  # comparing two conditions: SpikeNet and SourceSpikeNet\n",
    "\n",
    "# Create an instance of the TTestIndPower class\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "# Perform the power analysis to calculate the required sample size per group\n",
    "required_sample_size_per_group = power_analysis.solve_power(effect_size=effect_size, \n",
    "                                                            alpha=alpha, \n",
    "                                                            power=power)\n",
    "\n",
    "# Calculate the total sample size required\n",
    "total_sample_size = required_sample_size_per_group * num_groups\n",
    "\n",
    "# Print the results\n",
    "print(f\"Required sample size per group: {required_sample_size_per_group:.2f}\")\n",
    "print(f\"Total sample size required: {total_sample_size:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/mx3t8lrj4719csmgr9pw7n1r0000gr/T/ipykernel_65452/1616086225.py:7: DtypeWarning: Columns (4,5,6,8,10,13,14,15,18,19,20,21,22,23,24,28,29,30,31,32,33,34,35,36,38,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(f1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "f1 = '/Users/danielgoldenholz/icloud/Latest/LLM-derive-realEMR/daniel_g_project/meds_all.csv'\n",
    "f2 = '/Users/danielgoldenholz/icloud/Latest/LLM-derive-realEMR/MGB_Deidentified_Notes_March12th2024/mgb_notes_2015_metadata.csv'\n",
    "f1_hasnotes = '/Users/danielgoldenholz/icloud/Latest/LLM-derive-realEMR/daniel_g_project/meds_all_hasnotes.csv'\n",
    "\n",
    "# first make a reduced version with only has_notes==TRUE\n",
    "df1 = pd.read_csv(f1)\n",
    "\n",
    "df1_hasnotes = df1[df1['has_notes']==True]\n",
    "df1_hasnotes.to_csv(f1_hasnotes,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import time\n",
    "import gc   \n",
    "from sim_EMR_v2 import getLLM_response_withLLM\n",
    "\n",
    "def tryMe():\n",
    "    gc.collect() # clean up the memory\n",
    "    time_out = 120  # max seconds for generating one response. This is a safety feature to prevent too much output.\n",
    "    temp_sum = 1\n",
    "    #LLM_sum_name = 'llama3:8b'\n",
    "    LLM_sum_name = 'phi3:medium-128k'\n",
    "    # Create a client object\n",
    "    LLM_sum = Ollama(model=LLM_sum_name,temperature=temp_sum)\n",
    "    prompt = 'I have a 38 year old female with drug resistant epilepsy, hypothyroidism, and migraines. Explain what is the most likely to kill this patient and why.'\n",
    "    getLLM_response_withLLM(thePrompt=prompt,llm=LLM_sum,time_out=time_out)\n",
    "\n",
    "tryMe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120911044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c646213e2f0f4f91a0c48738d08ea73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13242081857_1031820141_20150706.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13242081857_1031820141_20150706.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13242081857_1031864355_20150706.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13242081857_1031864355_20150706.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13242081857_1031958478_20150706.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13242081857_1031958478_20150706.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13242081857_1031960301_20150706.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13242081857_1031960301_20150706.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036136474_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036136474_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13242081857_1036166032_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13242081857_1036166032_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036484449_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036484449_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036186172_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036186172_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036247454_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036247454_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036480458_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036480458_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036483866_20150708.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036483866_20150708.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13226473846_1036976251_20150709.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13226473846_1036976251_20150709.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes_13243564447_1040364020_20150712.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x 2015/Notes_13243564447_1040364020_20150712.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import tqdm.auto as tqdm\n",
    "from datetime import datetime\n",
    "#df2 = pd.read_csv(f2)\n",
    "\n",
    "# Define your array of filenames and tar archive name\n",
    "IDnum = 1\n",
    "print(df1_hasnotes.BDSPPatientID.iloc[IDnum])\n",
    "#print(df1_hasnotes.StartDTS.iloc[IDnum])\n",
    "ptID = df1_hasnotes.BDSPPatientID.iloc[IDnum]\n",
    "startDTS = df1_hasnotes.StartDTS.iloc[IDnum]\n",
    "endDTS = df1_hasnotes.EndDTS.iloc[IDnum]\n",
    "#startDTS = startDTS.rstrip('0')\n",
    "#endDTS = endDTS.rstrip('0')\n",
    "\n",
    "date_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "startDTS = startDTS.rstrip('0')\n",
    "endDTS = endDTS.rstrip('0')\n",
    "\n",
    "# If the last character is a dot, add one zero\n",
    "if startDTS[-1] == '.':\n",
    "    startDTS += '0'\n",
    "if endDTS[-1] == '.':\n",
    "    endDTS += '0'\n",
    "startDT = datetime.strptime(startDTS, date_format)\n",
    "endDT = datetime.strptime(endDTS,date_format)\n",
    "\n",
    "thisPt = df2[df2['BDSPPatientID']==ptID]\n",
    "tarfile = '/Users/danielgoldenholz/icloud/Latest/LLM-derive-realEMR/MGB_Deidentified_Notes_March12th2024/mgb_notes_2015.tar' \n",
    "output_dir='/Users/danielgoldenholz/icloud/Latest/LLM-derive-realEMR/MGB_Deidentified_Notes_March12th2024/'\n",
    "date_format2 = \"%Y%m%d\"\n",
    "\n",
    "\n",
    "# Loop through each filename and extract it from the tar file\n",
    "for index, fnameLine in tqdm.tqdm(thisPt.iterrows(), total=thisPt.shape[0]):\n",
    "    fname = fnameLine.DeidentifiedName\n",
    "    date_text = str(fnameLine.ContactDate)\n",
    "\n",
    "    # Convert the text to a datetime object\n",
    "    date_object = datetime.strptime(date_text, date_format2)\n",
    "\n",
    "    #print( str(startDT) + ' ' + str(date_object) + ' ' + str(endDT))\n",
    "    if date_object >= startDT and date_object <= endDT:\n",
    "        command = ['tar','-xvf',tarfile,'-C',output_dir,f'2015/{fname}']\n",
    "        subprocess.run(command)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deept1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
